# Blog Scraper Web Interface

A beautiful Next.js frontend for the blog scraper tool with real-time progress tracking and results visualization.

## Features

ğŸ¨ **Modern UI**: Clean, responsive interface built with Tailwind CSS
ğŸš€ **Real-time Progress**: Live updates during scraping operations  
ğŸ“Š **Results Dashboard**: Browse and download scraping results
âš™ï¸ **Configurable**: Adjust scraping parameters through the UI
ğŸ“± **Responsive**: Works on desktop, tablet, and mobile

## Screenshots

- **Dashboard**: Overview with stats and quick action buttons
- **Progress Tracking**: Real-time job monitoring with logs
- **Results Viewer**: Browse entries and download data
- **Configuration**: Adjust delays, retries, and other settings

## Setup Instructions

### 1. Install Dependencies

```bash
cd web-interface
npm install
```

### 2. Configure the Project

The frontend is configured to work with the Python scraper in the parent directory. Make sure your folder structure looks like this:

```
blog-scraper/
â”œâ”€â”€ src/                    # Python scraper source
â”œâ”€â”€ main.py                 # Python CLI
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ web-interface/         # Next.js frontend
    â”œâ”€â”€ src/
    â”œâ”€â”€ package.json
    â””â”€â”€ ...
```

### 3. Start the Development Server

```bash
npm run dev
```

The interface will be available at `http://localhost:3000`

## Usage

### Dashboard Overview

The main dashboard provides:

- **Active Jobs Counter**: Shows currently running scraping operations
- **Completed Jobs**: Historical results count
- **Total Entries**: Sum of all scraped entries

### Scraping Operations

Click any button to start scraping:

1. **ğŸš€ Scrape All Sources**: Complete scrape of all predefined sources
2. **ğŸ“„ interviewing.io**: Blog posts, company guides, interview guides  
3. **ğŸŸ£ Nil Mamano DSA**: Data structures & algorithms posts
4. **ğŸŒ Generic Blog**: Enter any blog URL to scrape
5. **ğŸ“„ PDF Document**: Local files or Google Drive links

### Configuration

Click the **Configure** button to adjust:

- **Delay Between Requests**: Respectful rate limiting (default: 1.0s)
- **Max Retries**: Retry failed requests (default: 3)
- **Timeout**: Request timeout in seconds (default: 30)
- **Max PDF Chapters**: Limit PDF extraction (default: 8)
- **Output Directory**: Where to save results (default: 'output')

### Progress Tracking

Active jobs show:

- **Real-time progress bar** with percentage
- **Live logs** from the Python scraper
- **Status indicators**: Pending â³, Running ğŸš€, Complete âœ…, Failed âŒ
- **Expandable log viewer** for debugging

### Results Viewer

Browse completed jobs:

- **Summary statistics** (found, scraped, errors)
- **Sample entries** with titles, content previews, and tags
- **Download button** to export results as JSON
- **Clickable entries** to view full details

## API Integration

The frontend communicates with the Python scraper through:

### `/api/scrape` (POST)

Starts a scraping job with streaming progress updates.

**Request Body:**
```json
{
  "type": "all" | "interviewing-io" | "nilmamano" | "generic-blog" | "pdf",
  "config": {
    "delay": 1.0,
    "maxRetries": 3,
    "timeout": 30,
    "maxChapters": 8,
    "blogUrl": "https://example.com/blog",
    "pdfUrl": "path/to/file.pdf",
    "outputDir": "output"
  },
  "jobId": "unique-job-id"
}
```

**Response:** Streaming JSON lines with progress updates and final results.

## Development

### Project Structure

```
web-interface/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ layout.tsx         # Root layout
â”‚   â”‚   â”œâ”€â”€ page.tsx          # Main dashboard
â”‚   â”‚   â”œâ”€â”€ globals.css       # Global styles
â”‚   â”‚   â””â”€â”€ api/
â”‚   â”‚       â””â”€â”€ scrape/
â”‚   â”‚           â””â”€â”€ route.ts  # API endpoint
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ ScrapingDashboard.tsx  # Main controls
â”‚   â”‚   â”œâ”€â”€ JobProgress.tsx        # Progress display
â”‚   â”‚   â”œâ”€â”€ ResultsViewer.tsx      # Results browser
â”‚   â”‚   â””â”€â”€ ConfigModal.tsx        # Settings modal
â”‚   â””â”€â”€ types/
â”‚       â””â”€â”€ index.ts          # TypeScript definitions
â”œâ”€â”€ package.json
â”œâ”€â”€ tailwind.config.js
â”œâ”€â”€ tsconfig.json
â””â”€â”€ next.config.js
```

### Build for Production

```bash
npm run build
npm start
```

### Technologies Used

- **Next.js 14**: React framework with App Router
- **TypeScript**: Type safety and better DX
- **Tailwind CSS**: Utility-first styling
- **Heroicons**: Beautiful SVG icons
- **React Hot Toast**: Toast notifications
- **date-fns**: Date formatting utilities

## Troubleshooting

### Common Issues

**Python script not found:**
- Ensure the Python scraper is in the parent directory
- Check the path in `/api/scrape/route.ts`

**Port already in use:**
```bash
# Kill process on port 3000
npx kill-port 3000
```

**Dependencies not found:**
```bash
# Clear cache and reinstall
rm -rf node_modules package-lock.json
npm install
```

**Scraping jobs fail immediately:**
- Check that Python dependencies are installed in the parent directory
- Verify `python3` is available in your PATH
- Check the console for detailed error messages

### Debug Mode

Enable verbose logging by checking the browser console and API logs:

```bash
# In the browser console
localStorage.setItem('debug', 'true')

# View API logs
# Check the terminal where you ran `npm run dev`
```

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the LICENSE file for details. 